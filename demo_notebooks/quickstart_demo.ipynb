{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splink demo \n",
    "\n",
    "In this demo we de-duplicate a small dataset.\n",
    "\n",
    "The purpose is to provide an end-to-end example of how to use the package\n",
    "\n",
    "I print the output at each stage using `spark_dataframe.show()`.  This is for instructional purposes only - it degrades performance and shouldn't be used in a production setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Imports and setup\n",
    "\n",
    "The following is just boilerplate code that sets up the Spark session and sets some other non-essential configuration options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig()  # Means logs will print in Jupyter Lab\n",
    "\n",
    "# Set to DEBUG if you want splink to log the SQL statements it's executing under the hood\n",
    "logging.getLogger(\"splink\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "conf=SparkConf()\n",
    "\n",
    "# Load in a jar that provides extended string comparison functions such as Jaro Winkler.\n",
    "# Splink \n",
    "conf.set('spark.driver.extraClassPath', 'jars/scala-udf-similarity-0.0.6.jar')\n",
    "conf.set('spark.jars', 'jars/scala-udf-similarity-0.0.6.jar')   \n",
    "\n",
    "\n",
    "# WARNING:\n",
    "# These config options are appropriate only if you're running Spark locally!!!\n",
    "conf.set('spark.driver.memory', '4g')\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"8\") \n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "\n",
    " # Register UDFs\n",
    "from pyspark.sql import types\n",
    "spark.udf.registerJavaFunction('jaro_winkler_sim', 'uk.gov.moj.dash.linkage.JaroWinklerSimilarity', types.DoubleType())\n",
    "spark.udf.registerJavaFunction('Dmetaphone', 'uk.gov.moj.dash.linkage.DoubleMetaphone', types.StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Configure splink using the `settings` object\n",
    "\n",
    "Most of `splink` configuration options are stored in a settings dictionary.  This dictionary allows significant customisation, and can therefore get quite complex.  \n",
    "\n",
    "Customisation overrides default values built into splink.  For the purposes of this demo, we will specify a simple settings dictionary, which means we will be relying on these sensible defaults.\n",
    "\n",
    "To help with authoring and validation of the settings dictionary, we have written a [json schema](https://json-schema.org/), which can be found [here](https://github.com/moj-analytical-services/splink/blob/dev/splink/files/settings_jsonschema.json).  We also provide an tool for helping to author valid settings dictionaries, which includes tooltips and autocomplete, which you can find [here](https://robinlinacre.com/simple_splink_settings_editor/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"proportion_of_matches\": 0.5,\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"blocking_rules\": [\n",
    "        'l.first_name = r.first_name',\n",
    "        'l.surname = r.surname',\n",
    "        'l.dob = r.dob'\n",
    "    ],\n",
    "    \"comparison_columns\": [\n",
    "        {\n",
    "            \"col_name\": \"first_name\",\n",
    "            \"num_levels\": 3,\n",
    "            \"term_frequency_adjustments\": True\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"surname\",\n",
    "            \"num_levels\": 3,\n",
    "            \"term_frequency_adjustments\": True\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"dob\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"city\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"email\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, this setting dictionary says:\n",
    "- We are performing a deduplication task (the other options are `link_only`, or `link_and_dedupe`)\n",
    "- The initial value (prior belief) for the proportion of matches amongst comparisons is 0.5 (50% of comparisons that result from the blocking rules are matches).\n",
    "- We are going generate comparisons subject to the blocking rules contained in the specified array\n",
    "- When comparing records, we will use information from the `first_name`, `surname`, `dob`, `city` and `email` columns to compute a match score.\n",
    "- For `first_name` and `surname`, string comparisons will have three levels:\n",
    "    - Level 2: Strings are (almost) exactly the same\n",
    "    - Level 1: Strings are similar \n",
    "    - Level 0: No match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"data/fake_1000.parquet\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:  Estimate match scores using the Expectation Maximisation algorithm\n",
    "Columns are assumed to be strings by default.  See the 'comparison vector settings' notebook for details of configuration options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink import Splink\n",
    "\n",
    "linker = Splink(settings, spark, df=df)\n",
    "df_e = linker.get_scored_comparisons()\n",
    "df_e.persist()  # Later, we will make term frequency adjustments.  Persist caches these results in memory, preventing them having to be recomputed when we make these adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inspect results \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect main dataframe that contains the match scores\n",
    "df_e.toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `params` property of the `linker` is an object that contains a lot of diagnostic information about how the match probability was computed.  The following cells demonstrate some of its functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = linker.params\n",
    "params.probability_distribution_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative representation of the parameters displays them in terms of the effect different values in the comparison vectors have on the match probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.adjustment_factor_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If charts aren't displaying correctly in your notebook, you can write them to a file (by default splink_charts.html)\n",
    "params.all_charts_write_html_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also generate a report which explains how the match probability was computed for an individual comparison row.  \n",
    "\n",
    "Note that you need to convert the row to a dictionary for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink.intuition import intuition_report\n",
    "row_dict = df_e.toPandas().sample(1).to_dict(orient=\"records\")[0]\n",
    "print(intuition_report(row_dict, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Term frequency adjustments\n",
    "\n",
    "Splink enables you to make adjustments for term frequency on any number of columns\n",
    "\n",
    "This enables match probabilities to be adjusted for e.g. the fact John Smith is more prevalent than Robin Linacre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e_adj = linker.make_term_frequency_adjustments(df_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdtf = df_e_adj.toPandas()\n",
    "sam = pdtf.sample(10)\n",
    "sam[[\"match_probability\", \"tf_adjusted_match_prob\"] + list(pdtf.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
