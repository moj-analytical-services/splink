from copy import deepcopy

from .blocking import block_using_rules
from .comparison_vector_values import compute_comparison_vector_values
from .maximisation_step import _compute_new_parameters


def _num_target_rows_to_rows_to_sample(target_rows):
    # Number of rows generated by cartesian product is
    # n(n-1)/2, where n is input rows
    # We want to set a target_rows = t, the number of
    # rows generated by Splink and find out how many input rows
    # we need to generate targer rows
    #     Solve t = n(n-1)/2 for n
    #     https://www.wolframalpha.com/input/?i=Solve%5Bt%3Dn+*+%28n+-+1%29+%2F+2%2C+n%5D
    sample_rows = 0.5 * ((8 * target_rows + 1) ** 0.5 + 1)
    return sample_rows


def estimate_u_values(linker, sql_pipeline, target_rows):
    import time
    t = time.time()
    start = time.time()
    original_settings_object = linker.settings_obj
    settings_obj = deepcopy(linker.settings_obj)
    settings_obj._retain_matching_columns = False
    settings_obj._retain_intermediate_calculation_columns = False
    for cc in settings_obj.comparisons:
        for cl in cc.comparison_levels:
            cl._level_dict["tf_adjustment_column"] = None
    print("--- Settings stuff... %s seconds ---" % (time.time() - t))
    t = time.time()

    # might not even need to the linker.sql_tracker stuff...
    sql = f"""
    select count(*) as count
    from '{linker.sql_tracker["__splink__df_concat"][0]}'
    """

    result = linker.execute_sql({"sql_pipe": [sql], "prev_dfs": ["__splink__df_concat"]})
    count_rows = result["count"][0]
    print("--- Count Rows... %s seconds ---" % (time.time() - t))
    t = time.time()

    if settings_obj._link_type in ["dedupe_only", "link_and_dedupe"]:
        sample_size = _num_target_rows_to_rows_to_sample(target_rows)
        proportion = sample_size / count_rows

    if settings_obj._link_type == "link_only":
        sample_size = target_rows**0.5
        proportion = sample_size / count_rows

    if proportion >= 1.0:
        proportion = 1.0

    if sample_size > count_rows:
        sample_size = count_rows

    # might not even need to tweak this...
    sql = f"""
    select *
    from "__splink__df_concat"
    {linker.random_sample_sql(proportion, sample_size)}
    """

    linker.generate_sql(
        sql, sql_pipeline, "__splink__df_concat_with_tf", transpile=False
    )

    settings_obj._blocking_rules_to_generate_predictions = []
    print("--- Generate __splink__df_concat_with_tf_2... %s seconds ---" % (time.time() - t))
    t = time.time()

    sql_pipeline = block_using_rules(settings_obj, sql_pipeline, linker.generate_sql)

    sql_pipeline = compute_comparison_vector_values(
        settings_obj, sql_pipeline, linker.generate_sql
    )
    print("--- compute_comparison_vector_values... %s seconds ---" % (time.time() - t))
    t = time.time()

    sql = """
    select *, 0.0D as match_probability
    from __splink__df_comparison_vectors
    """
    sql_pipeline = linker.generate_sql(sql, sql_pipeline, "__splink__df_predict")

    sql_pipeline = _compute_new_parameters(settings_obj, sql_pipeline, linker.generate_sql)
    param_records = linker.execute_sql(sql_pipeline).to_dict(orient="records")
    print("--- execute_sql... %s seconds ---" % (time.time() - t))
    t = time.time()

    m_u_records = [
        r for r in param_records if r["comparison_name"] != "_proportion_of_matches"
    ]

    for record in m_u_records:
        cc = original_settings_object._get_comparison_by_name(record["comparison_name"])
        gamma_val = record["comparison_vector_value"]
        cl = cc.get_comparison_level_by_comparison_vector_value(gamma_val)

        cl.add_trained_u_probability(
            record["u_probability"], "estimate u by random sampling"
        )
    print("--- For loop... %s seconds ---" % (time.time() - t))
    print("--- Total... %s seconds ---" % (time.time() - start))