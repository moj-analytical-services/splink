from __future__ import annotations

import logging
from collections import defaultdict
from copy import deepcopy
from typing import TYPE_CHECKING, List

import pandas as pd

from splink.internals.blocking import (
    block_using_rules_sqls,
    BlockingRule,
)
from splink.internals.comparison_vector_values import (
    compute_comparison_vector_values_from_id_pairs_sqls,
)
from splink.internals.m_u_records_to_parameters import (
    append_u_probability_to_comparison_level_trained_probabilities,
    m_u_records_to_lookup_dict,
)
from splink.internals.pipeline import CTEPipeline
from splink.internals.settings import Settings
from splink.internals.vertically_concatenate import (
    enqueue_df_concat,
    split_df_concat_with_tf_into_two_tables_sqls,
)

from .expectation_maximisation import (
    compute_new_parameters_sql,
    compute_proportions_for_new_parameters,
)

# https://stackoverflow.com/questions/39740632/python-type-hinting-without-cyclic-imports
if TYPE_CHECKING:
    from splink.internals.linker import Linker

logger = logging.getLogger(__name__)


def _rows_needed_for_n_pairs(n_pairs):
    # Number of pairs generated by cartesian product is
    # p(r) = r(r-1)/2, where r is input rows
    # Solve this for r
    # https://www.wolframalpha.com/input?i=Solve%5Bp%3Dr+*+%28r+-+1%29+%2F+2%2C+r%5D
    sample_rows = 0.5 * ((8 * n_pairs + 1) ** 0.5 + 1)
    return sample_rows


def _proportion_sample_size_link_only(
    row_counts_individual_dfs: List[int], max_pairs: float
) -> tuple[float, float]:
    # total valid links is sum of pairwise product of individual row counts
    # i.e. if frame_counts are [a, b, c, d, ...],
    # total_links = a*b + a*c + a*d + ... + b*c + b*d + ... + c*d + ...
    total_links = (
        sum(row_counts_individual_dfs) ** 2
        - sum([count**2 for count in row_counts_individual_dfs])
    ) / 2
    total_nodes = sum(row_counts_individual_dfs)

    # if we scale each frame by a proportion total_links scales with the square
    # i.e. (our target) max_pairs == proportion^2 * total_links
    proportion = (max_pairs / total_links) ** 0.5
    # sample size is for df_concat_with_tf, i.e. proportion of the total nodes
    sample_size = proportion * total_nodes
    return proportion, sample_size


def estimate_u_values(linker: Linker, max_pairs: float, seed: int = None) -> None:
    logger.info("----- Estimating u probabilities using random sampling -----")
    pipeline = CTEPipeline()

    pipeline = enqueue_df_concat(linker, pipeline)

    original_settings_obj = linker._settings_obj

    training_linker: Linker = deepcopy(linker)

    settings_obj = training_linker._settings_obj
    settings_obj._retain_matching_columns = False
    settings_obj._retain_intermediate_calculation_columns = False

    db_api = training_linker._db_api

    for cc in settings_obj.comparisons:
        for cl in cc.comparison_levels:
            # TODO: ComparisonLevel: manage access
            cl._tf_adjustment_column = None

    if settings_obj._link_type in ["dedupe_only", "link_and_dedupe"]:
        sql = """
        select count(*) as count
        from __splink__df_concat
        """

        pipeline.enqueue_sql(sql, "__splink__df_concat_count")
        count_dataframe = db_api.sql_pipeline_to_splink_dataframe(pipeline)

        result = count_dataframe.as_record_dict()
        count_dataframe.drop_table_from_database_and_remove_from_cache()
        total_nodes = result[0]["count"]
        sample_size = _rows_needed_for_n_pairs(max_pairs)
        proportion = sample_size / total_nodes

    if settings_obj._link_type == "link_only":
        sql = """
        select count(source_dataset) as count
        from __splink__df_concat
        group by source_dataset
        """
        pipeline.enqueue_sql(sql, "__splink__df_concat_count")
        counts_dataframe = db_api.sql_pipeline_to_splink_dataframe(pipeline)
        result = counts_dataframe.as_record_dict()
        counts_dataframe.drop_table_from_database_and_remove_from_cache()
        frame_counts = [res["count"] for res in result]

        proportion, sample_size = _proportion_sample_size_link_only(
            frame_counts, max_pairs
        )

        total_nodes = sum(frame_counts)

    if proportion >= 1.0:
        proportion = 1.0

    if sample_size > total_nodes:
        sample_size = total_nodes

    table_to_sample_from = "__splink__df_concat"
    # if we are provided a seed, we want to order the table before we sample from it
    # this ensures that the resulting table will be consistent across runs
    # (which is what we want when we are supplying a seed)
    # don't bother when we aren't using a seed as it is needless computation
    if seed is not None:
        uid_colname = settings_obj.column_info_settings.unique_id_input_column.name
        table_to_sample_from = (
            f"(select * from {table_to_sample_from} order by {uid_colname})"
        )

    pipeline = CTEPipeline()
    pipeline = enqueue_df_concat(training_linker, pipeline)

    sql = f"""
    select *
    from {table_to_sample_from}
    {training_linker._random_sample_sql(proportion, sample_size, seed)}
    """

    pipeline.enqueue_sql(sql, "__splink__df_concat_sample")
    df_sample = db_api.sql_pipeline_to_splink_dataframe(pipeline)

    settings_obj._blocking_rules_to_generate_predictions = []

    input_tablename_sample_l = "__splink__df_concat_sample"
    input_tablename_sample_r = "__splink__df_concat_sample"

    split_sqls: list[dict[str, str]] = []

    if (
        len(linker._input_tables_dict) == 2
        and linker._settings_obj._link_type == "link_only"
    ):
        split_sqls = split_df_concat_with_tf_into_two_tables_sqls(
            "__splink__df_concat",
            linker._settings_obj.column_info_settings.source_dataset_column_name,
            sample_switch=True,
        )
        input_tablename_sample_l = "__splink__df_concat_sample_left"
        input_tablename_sample_r = "__splink__df_concat_sample_right"

    # We chunk only the RHS. Start with a hardcoded chunk count.
    rhs_num_chunks = 10

    uid_columns = settings_obj.column_info_settings.unique_id_input_columns
    source_dataset_col = settings_obj.column_info_settings.source_dataset_input_column
    uid_col = settings_obj.column_info_settings.unique_id_input_column

    # Build common blocking columns (UID columns that all comparisons need)
    common_blocking_cols: list[str] = []
    for uid_column in uid_columns:
        common_blocking_cols.extend(uid_column.l_r_names_as_l_r)

    for i, comparison in enumerate(settings_obj.comparisons):
        logger.info(
            f"\nEstimating u for: {comparison.output_column_name} "
            f"({i+1}/{len(settings_obj.comparisons)})"
        )
        original_comparison = original_settings_obj.comparisons[i]

        # Keep a running total of m/u counts across RHS chunks.
        # Keyed by (output_column_name, comparison_vector_value).
        counts_lookup: defaultdict[tuple[str, int], list[float]] = defaultdict(
            lambda: [0.0, 0.0]
        )

        for rhs_chunk_num in range(1, rhs_num_chunks + 1):
            logger.info(f"  RHS chunk {rhs_chunk_num}/{rhs_num_chunks}")

            pipeline = CTEPipeline(input_dataframes=[df_sample])

            if split_sqls:
                pipeline.enqueue_list_of_sqls(split_sqls)

            # Ensure chunking uses the correct backend dialect, even when there are
            # no user-provided blocking rules.
            blocking_rules = [
                BlockingRule("1=1", sql_dialect_str=db_api.sql_dialect.sql_dialect_str)
            ]

            blocking_sqls = block_using_rules_sqls(
                input_tablename_l=input_tablename_sample_l,
                input_tablename_r=input_tablename_sample_r,
                blocking_rules=blocking_rules,
                link_type=linker._settings_obj._link_type,
                source_dataset_input_column=settings_obj.column_info_settings.source_dataset_input_column,
                unique_id_input_column=settings_obj.column_info_settings.unique_id_input_column,
                right_chunk=(rhs_chunk_num, rhs_num_chunks),
            )

            # Persist each chunk's blocked pairs under a unique name, but keep
            # `__splink__blocked_id_pairs` available for downstream SQL.
            chunk_blocked_pairs_table = (
                f"__splink__blocked_id_pairs_R{rhs_chunk_num}of{rhs_num_chunks}"
            )
            for s in blocking_sqls:
                if s.get("output_table_name") == "__splink__blocked_id_pairs":
                    s["output_table_name"] = chunk_blocked_pairs_table

            pipeline.enqueue_list_of_sqls(blocking_sqls)

            sql = f"select * from {chunk_blocked_pairs_table}"
            pipeline.enqueue_sql(sql, "__splink__blocked_id_pairs")

            # Blocking needs UIDs + comparison-specific columns
            blocking_cols = (
                common_blocking_cols + comparison._columns_to_select_for_blocking()
            )

            # Comparison vector needs UIDs + comparison output + match_key
            cv_cols = Settings.columns_to_select_for_comparison_vector_values(
                unique_id_input_columns=uid_columns,
                comparisons=[comparison],
                retain_matching_columns=False,
                additional_columns_to_retain=[],
            )

            cv_sqls = compute_comparison_vector_values_from_id_pairs_sqls(
                blocking_cols,
                cv_cols,
                input_tablename_l=input_tablename_sample_l,
                input_tablename_r=input_tablename_sample_r,
                source_dataset_input_column=source_dataset_col,
                unique_id_input_column=uid_col,
            )

            pipeline.enqueue_list_of_sqls(cv_sqls)

            # Add dummy match_probability column required by compute_new_parameters_sql
            sql = """
            select *, cast(0.0 as float8) as match_probability
            from __splink__df_comparison_vectors
            """
            pipeline.enqueue_sql(sql, "__splink__df_predict")

            # Compute u probability counts for this comparison and chunk
            sql = compute_new_parameters_sql(
                estimate_without_term_frequencies=False,
                comparisons=[comparison],
            )
            pipeline.enqueue_sql(sql, "__splink__m_u_counts")

            df_params = db_api.sql_pipeline_to_splink_dataframe(pipeline)
            chunk_counts = df_params.as_pandas_dataframe()
            df_params.drop_table_from_database_and_remove_from_cache()

            # Drop lambda row: it isn't additive across chunks (it's already a
            # proportion), and we don't use it here anyway.
            chunk_counts = chunk_counts[
                chunk_counts.output_column_name
                != "_probability_two_random_records_match"
            ]

            for r in chunk_counts.itertuples(index=False):
                key = (r.output_column_name, int(r.comparison_vector_value))
                totals = counts_lookup[key]
                totals[0] += float(r.m_count)
                totals[1] += float(r.u_count)

        aggregated_counts_df = pd.DataFrame(
            [
                {
                    "output_column_name": ocn,
                    "comparison_vector_value": cvv,
                    "m_count": totals[0],
                    "u_count": totals[1],
                }
                for (ocn, cvv), totals in counts_lookup.items()
            ]
        )

        # Convert aggregated counts to proportions (u probabilities)
        param_records = compute_proportions_for_new_parameters(aggregated_counts_df)

        m_u_records = param_records
        m_u_records_lookup = m_u_records_to_lookup_dict(m_u_records)

        # Apply estimated u values to the original settings object
        for cl in original_comparison._comparison_levels_excluding_null:
            append_u_probability_to_comparison_level_trained_probabilities(
                cl,
                m_u_records_lookup,
                original_comparison.output_column_name,
                "estimate u by random sampling",
            )

    df_sample.drop_table_from_database_and_remove_from_cache()

    logger.info("\nEstimated u probabilities using random sampling")
