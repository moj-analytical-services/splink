from __future__ import annotations

import logging
import time
from copy import deepcopy
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, List, Optional

from splink.internals.blocking import (
    _sql_gen_where_condition,
    backend_link_type_options,
    combine_unique_id_input_columns,
)
from splink.internals.comparison import Comparison
from splink.internals.comparison_level import M_U_CLAMP_MIN
from splink.internals.comparison_vector_values import (
    compute_comparison_vector_values_from_id_pairs_sqls,
)
from splink.internals.input_column import InputColumn
from splink.internals.m_u_records_to_parameters import (
    append_u_probability_to_comparison_level_trained_probabilities,
    m_u_records_to_lookup_dict,
)
from splink.internals.pipeline import CTEPipeline
from splink.internals.settings import Settings
from splink.internals.splink_dataframe import SplinkDataFrame
from splink.internals.unique_id_concat import _composite_unique_id_from_nodes_sql
from splink.internals.vertically_concatenate import (
    enqueue_df_concat,
    split_df_concat_with_tf_into_two_tables_sqls,
)

from .database_api import DatabaseAPISubClass

# https://stackoverflow.com/questions/39740632/python-type-hinting-without-cyclic-imports
if TYPE_CHECKING:
    from splink.internals.linker import Linker

logger = logging.getLogger(__name__)


@dataclass
class LevelConvergenceInfo:
    """Diagnostics for a single comparison level's convergence."""

    comparison_vector_value: int
    label: str
    count: int
    u_probability: float
    converged: bool


@dataclass
class ComparisonConvergenceInfo:
    """Diagnostics for a comparison's u estimation convergence."""

    output_column_name: str
    total_pairs_sampled: int
    batches_processed: int
    converged: bool
    time_seconds: float
    levels: List[LevelConvergenceInfo] = field(default_factory=list)


def _rows_needed_for_n_pairs(n_pairs: float) -> float:
    # Number of pairs generated by cartesian product is
    # p(r) = r(r-1)/2, where r is input rows
    # Solve this for r
    # https://www.wolframalpha.com/input?i=Solve%5Bp%3Dr+*+%28r+-+1%29+%2F+2%2C+r%5D
    sample_rows = 0.5 * ((8 * n_pairs + 1) ** 0.5 + 1)
    return sample_rows


def _proportion_sample_size_link_only(
    row_counts_individual_dfs: List[int], max_pairs: float
) -> tuple[float, float]:
    # total valid links is sum of pairwise product of individual row counts
    # i.e. if frame_counts are [a, b, c, d, ...],
    # total_links = a*b + a*c + a*d + ... + b*c + b*d + ... + c*d + ...
    total_links = (
        sum(row_counts_individual_dfs) ** 2
        - sum([count**2 for count in row_counts_individual_dfs])
    ) / 2
    total_nodes = sum(row_counts_individual_dfs)

    # if we scale each frame by a proportion total_links scales with the square
    # i.e. (our target) max_pairs == proportion^2 * total_links
    proportion = (max_pairs / total_links) ** 0.5
    # sample size is for df_concat_with_tf, i.e. proportion of the total nodes
    sample_size = proportion * total_nodes
    return proportion, sample_size


def _generate_row_random_sql(
    unique_id_columns: List[InputColumn],
    seed: Optional[int],
    db_api: DatabaseAPISubClass,
) -> str:
    """Generate SQL expression for a deterministic random value in [0, 1) for each row.

    Uses a hash of the row ID and seed to produce deterministic random values.
    This ensures reproducibility and that the random values don't change when
    materialized tables are re-read in distributed settings.
    """
    # Build a deterministic random from hash of ID + seed
    composite_id = _composite_unique_id_from_nodes_sql(unique_id_columns)
    seed_val = seed if seed is not None else 42
    seed_str = str(seed_val)
    hash_input = f"cast({composite_id} as varchar) || '{seed_str}'"
    hash_sql = db_api.sql_dialect.hash_function_expression(hash_input)
    # Map hash to [0, 1) range using modulo
    return f"abs({hash_sql} % 1000000) / 1000000.0"


def _run_batch_for_comparison(
    comparison: Comparison,
    db_api: DatabaseAPISubClass,
    df_sample_with_rand: SplinkDataFrame,
    left_table_name: str,
    right_table_name: str,
    uid_input_columns: List[InputColumn],
    source_ds_column: Optional[InputColumn],
    uid_column: InputColumn,
    df_left: Optional[SplinkDataFrame],
    df_right: Optional[SplinkDataFrame],
    prev_row_threshold: float,
    curr_row_threshold: float,
    link_type: backend_link_type_options,
) -> dict[int, int]:
    """Run a single batch and return counts per comparison level.

    Batching is done by filtering rows based on __splink_u_rand thresholds.
    Pairs are generated on-the-fly from the filtered rows, avoiding materialization
    of O(n²) blocked pairs.

    The batch selection logic ensures disjoint batches:
    - Both rows must have rand < curr_threshold (to be in the cumulative sample)
    - At least one row must have rand >= prev_threshold (to be new in this batch)
    """
    gamma_column = comparison._gamma_column_name

    # Build columns needed for comparison vector computation
    blocking_cols = []
    for uid_col in uid_input_columns:
        blocking_cols.extend(uid_col.l_r_names_as_l_r)
    blocking_cols.extend(comparison._columns_to_select_for_blocking())

    cv_cols = Settings.columns_to_select_for_comparison_vector_values(
        unique_id_input_columns=uid_input_columns,
        comparisons=[comparison],
        retain_matching_columns=False,
        additional_columns_to_retain=[],
    )

    # Set up input dataframes for the pipeline
    pipeline_inputs = [df_sample_with_rand]
    if df_left is not None:
        pipeline_inputs.append(df_left)
    if df_right is not None:
        pipeline_inputs.append(df_right)

    pipeline = CTEPipeline(pipeline_inputs)

    # Generate the where condition for link type
    unique_id_columns = combine_unique_id_input_columns(source_ds_column, uid_column)

    where_condition = _sql_gen_where_condition(
        link_type,
        unique_id_columns,
    )

    uid_l_expr = _composite_unique_id_from_nodes_sql(unique_id_columns, "l")
    uid_r_expr = _composite_unique_id_from_nodes_sql(unique_id_columns, "r")

    # Check if we're using separate left/right tables (link_only with 2 inputs)
    using_separate_tables = left_table_name != right_table_name

    if using_separate_tables:
        # For link_only mode with separate tables:
        # Filter each table independently, then join
        filter_sql_l_cumulative = f"""
        select * from {left_table_name}
        where __splink_u_rand < {curr_row_threshold}
        """
        pipeline.enqueue_sql(filter_sql_l_cumulative, "__splink__u_rows_l_cumulative")

        filter_sql_l_new = f"""
        select * from {left_table_name}
        where __splink_u_rand >= {prev_row_threshold}
          and __splink_u_rand < {curr_row_threshold}
        """
        pipeline.enqueue_sql(filter_sql_l_new, "__splink__u_rows_l_new")

        filter_sql_r_cumulative = f"""
        select * from {right_table_name}
        where __splink_u_rand < {curr_row_threshold}
        """
        pipeline.enqueue_sql(filter_sql_r_cumulative, "__splink__u_rows_r_cumulative")

        filter_sql_r_new = f"""
        select * from {right_table_name}
        where __splink_u_rand >= {prev_row_threshold}
          and __splink_u_rand < {curr_row_threshold}
        """
        pipeline.enqueue_sql(filter_sql_r_new, "__splink__u_rows_r_new")

        # Generate pairs where at least one side is new:
        # 1. new_left × cumulative_right
        # 2. cumulative_left × new_right (but left not new, to avoid double counting)
        blocked_pairs_sql = f"""
        select
            '0' as match_key,
            {uid_l_expr} as join_key_l,
            {uid_r_expr} as join_key_r
        from __splink__u_rows_l_new as l
        inner join __splink__u_rows_r_cumulative as r
        on 1=1
        {where_condition}
        UNION ALL
        select
            '0' as match_key,
            {uid_l_expr} as join_key_l,
            {uid_r_expr} as join_key_r
        from __splink__u_rows_l_cumulative as l
        inner join __splink__u_rows_r_new as r
        on 1=1
        {where_condition}
        and l.__splink_u_rand < {prev_row_threshold}
        """

        # For CV computation, we need to use the cumulative tables
        cv_left_table = "__splink__u_rows_l_cumulative"
        cv_right_table = "__splink__u_rows_r_cumulative"

    else:
        # For dedupe/link_and_dedupe with single table:
        # Filter the single table for both sides
        filter_sql_cumulative = f"""
        select * from {left_table_name}
        where __splink_u_rand < {curr_row_threshold}
        """
        pipeline.enqueue_sql(filter_sql_cumulative, "__splink__u_rows_cumulative")

        filter_sql_new = f"""
        select * from {left_table_name}
        where __splink_u_rand >= {prev_row_threshold}
          and __splink_u_rand < {curr_row_threshold}
        """
        pipeline.enqueue_sql(filter_sql_new, "__splink__u_rows_new")

        # Generate pairs where at least one side is new:
        # 1. new × cumulative
        # 2. cumulative × new (but left not new, to avoid double counting)
        blocked_pairs_sql = f"""
        select
            '0' as match_key,
            {uid_l_expr} as join_key_l,
            {uid_r_expr} as join_key_r
        from __splink__u_rows_new as l
        inner join __splink__u_rows_cumulative as r
        on 1=1
        {where_condition}
        UNION ALL
        select
            '0' as match_key,
            {uid_l_expr} as join_key_l,
            {uid_r_expr} as join_key_r
        from __splink__u_rows_cumulative as l
        inner join __splink__u_rows_new as r
        on 1=1
        {where_condition}
        and l.__splink_u_rand < {prev_row_threshold}
        """

        cv_left_table = "__splink__u_rows_cumulative"
        cv_right_table = "__splink__u_rows_cumulative"

    pipeline.enqueue_sql(blocked_pairs_sql, "__splink__blocked_id_pairs")

    # Compute comparison vectors for this batch
    cv_sqls = compute_comparison_vector_values_from_id_pairs_sqls(
        blocking_cols,
        cv_cols,
        input_tablename_l=cv_left_table,
        input_tablename_r=cv_right_table,
        source_dataset_input_column=source_ds_column,
        unique_id_input_column=uid_column,
    )
    pipeline.enqueue_list_of_sqls(cv_sqls)

    # Aggregate counts by gamma level
    count_sql = f"""
    select {gamma_column} as cv_value, count(*) as cv_count
    from __splink__df_comparison_vectors
    group by {gamma_column}
    """
    pipeline.enqueue_sql(count_sql, "__splink__u_level_counts")

    df_result = db_api.sql_pipeline_to_splink_dataframe(pipeline)
    result_rows = df_result.as_record_dict()
    df_result.drop_table_from_database_and_remove_from_cache()

    # Convert to dict, ensuring all comparison levels are present (even with 0 count)
    # SQL GROUP BY only returns rows with at least 1 observation, so we need to
    # initialize all expected cv_values with 0 first
    result_dict: dict[int, int] = {
        cl.comparison_vector_value: 0
        for cl in comparison._comparison_levels_excluding_null
    }
    # Update with actual counts from SQL result
    for row in result_rows:
        cv_val = row["cv_value"]
        if cv_val in result_dict:  # Only update non-null levels
            result_dict[cv_val] = row["cv_count"]

    # Log the complete result including levels with 0 count
    import pandas as pd

    log_df = pd.DataFrame(sorted(result_dict.items()), columns=["cv_value", "cv_count"])
    # Log at debug level to reduce noise since we provide summaries at INFO level
    logger.log(5, log_df.to_markdown(index=False))

    return result_dict


def _estimate_u_with_row_batching(
    comparison: Comparison,
    db_api: DatabaseAPISubClass,
    df_sample_with_rand: SplinkDataFrame,
    left_table_name: str,
    right_table_name: str,
    uid_input_columns: List[InputColumn],
    source_ds_column: Optional[InputColumn],
    uid_column: InputColumn,
    df_left: Optional[SplinkDataFrame],
    df_right: Optional[SplinkDataFrame],
    target_count_per_level: int,
    min_pairs: int,
    starting_row_proportion: float,
    row_proportion_growth: float,
    link_type: backend_link_type_options,
) -> ComparisonConvergenceInfo:
    """Estimate u probabilities using row-based batching with convergence checking.

    Instead of materializing all pairs, we grow the row sample iteratively.
    Pairs are generated lazily from filtered rows in each batch.

    The number of pairs scales quadratically with the row proportion:
    - 10% of rows → ~1% of pairs
    - 30% of rows → ~9% of pairs
    - 100% of rows → 100% of pairs

    This provides natural geometric growth of pair counts while only
    materializing O(n) rows, not O(n²) pairs.
    """
    start_ts = time.time()

    levels = comparison._comparison_levels_excluding_null
    accumulated_counts: dict[int, int] = {
        cl.comparison_vector_value: 0 for cl in levels
    }
    pair_count = 0
    batch_num = 0

    prev_threshold = 0.0
    step_size = starting_row_proportion
    is_converged = False
    min_pairs_reached = False

    # Initial log about the batching strategy
    logger.info("")
    logger.info(f"  Starting row-based batching for '{comparison.output_column_name}'")
    logger.info(
        f"    Target: {target_count_per_level:,} samples/level, "
        f"{min_pairs:,} min pairs, "
        f"starting with {starting_row_proportion:.1%} of rows"
    )

    while True:
        curr_threshold = min(prev_threshold + step_size, 1.0)
        rows_pct = curr_threshold * 100

        logger.info("")
        logger.info(f"  Batch {batch_num + 1}: Sampling {rows_pct:.1f}% of rows...")

        batch_counts = _run_batch_for_comparison(
            comparison=comparison,
            db_api=db_api,
            df_sample_with_rand=df_sample_with_rand,
            left_table_name=left_table_name,
            right_table_name=right_table_name,
            uid_input_columns=uid_input_columns,
            source_ds_column=source_ds_column,
            uid_column=uid_column,
            df_left=df_left,
            df_right=df_right,
            prev_row_threshold=prev_threshold,
            curr_row_threshold=curr_threshold,
            link_type=link_type,
        )

        # Accumulate results
        batch_pair_total = 0
        for cv_val, cnt in batch_counts.items():
            batch_pair_total += cnt
            if cv_val in accumulated_counts:
                accumulated_counts[cv_val] += cnt

        pair_count += batch_pair_total
        batch_num += 1

        # Check if all levels have enough samples
        # Every level must have at least target_count_per_level observations
        # If a level has 0 observations, it's NOT converged (0 < target_count)
        # The loop will continue until either all levels converge OR we've
        # sampled all data (curr_threshold >= 1.0)
        levels_converged = all(
            cnt >= target_count_per_level for cnt in accumulated_counts.values()
        )
        min_pairs_reached = pair_count >= min_pairs
        is_converged = levels_converged and min_pairs_reached

        # Detailed convergence logging
        logger.info(
            f"    Result: +{batch_pair_total:,} pairs this batch, "
            f"{pair_count:,} total"
        )

        if not min_pairs_reached:
            logger.info(f"    ✗ min_pairs not met ({pair_count:,} < {min_pairs:,})")

        if not levels_converged:
            # Find levels that haven't converged and get their labels
            unconverged = []
            for cv in sorted(accumulated_counts.keys()):
                if accumulated_counts[cv] < target_count_per_level:
                    # Get the correct level from the comparison
                    level = comparison._get_comparison_level_by_comparison_vector_value(
                        cv
                    )
                    # Truncate label if too long
                    label = level.label_for_charts
                    if len(label) > 50:
                        label = label[:47] + "..."
                    unconverged.append((cv, accumulated_counts[cv], label))

            unconverged_str = ", ".join(
                f"L{cv}:{cnt:,}/{target_count_per_level:,} ({label})"
                for cv, cnt, label in unconverged
            )
            logger.info(f"    ✗ Levels below threshold: {unconverged_str}")

        if is_converged:
            logger.info("")
            logger.info(f"  ✓ CONVERGED after {batch_num} batches")
            break

        if curr_threshold >= 1.0:
            logger.info("")
            logger.info(
                f"  ! Reached 100% of data after {batch_num} batches "
                f"without convergence"
            )
            break

        # Calculate next step size, but if it would exceed 80%, jump to 100%
        next_step_size = step_size * row_proportion_growth
        # Fix: use curr_threshold (where we are now), not prev_threshold
        next_threshold = min(curr_threshold + next_step_size, 1.0)

        # If next batch would sample > 80%, skip to 100% instead
        if next_threshold > 0.8 and next_threshold < 1.0:
            logger.info("    → Jumping to 100% (next step would exceed 80%)")
            # Set step_size so that curr_threshold + step_size = 1.0
            step_size = 1.0 - curr_threshold
        else:
            next_pct = next_threshold * 100
            logger.info(f"    → Next batch: {next_pct:.1f}% of rows")
            step_size = next_step_size

        prev_threshold = curr_threshold

    elapsed = time.time() - start_ts

    # Build diagnostics
    level_results = []
    for cl in levels:
        cv = cl.comparison_vector_value
        level_cnt = accumulated_counts[cv]
        u_val = level_cnt / pair_count if pair_count > 0 else M_U_CLAMP_MIN
        u_val = max(u_val, M_U_CLAMP_MIN)
        level_results.append(
            LevelConvergenceInfo(
                comparison_vector_value=cv,
                label=cl.label_for_charts,
                count=level_cnt,
                u_probability=u_val,
                converged=level_cnt >= target_count_per_level,
            )
        )

    return ComparisonConvergenceInfo(
        output_column_name=comparison.output_column_name,
        total_pairs_sampled=pair_count,
        batches_processed=batch_num,
        converged=is_converged,
        time_seconds=elapsed,
        levels=level_results,
    )


def estimate_u_values(
    linker: Linker,
    max_pairs: float,
    seed: Optional[int] = None,
    min_count: int = 100,
    min_pairs: int = 1_000_000,
    initial_batch_proportion: Optional[float] = None,
    batch_growth_factor: float = 2.0,
) -> List[ComparisonConvergenceInfo]:
    """Estimate u probabilities using random sampling with count-based convergence.

    Uses row-based batching: instead of materializing all candidate pairs,
    iteratively grows a row sample and generates pairs on-the-fly. This avoids
    materializing O(n²) pairs, only materializing O(n) sampled rows.

    For each comparison, iteratively samples batches of pairs and counts how many
    fall into each comparison level. Stops when:
    1. All levels have at least `min_count` observations, AND
    2. At least `min_pairs` total pairs have been sampled, OR
    3. All rows have been included (dataset exhausted)

    Args:
        linker: The Splink linker object.
        max_pairs: Target number of pairs (controls row sample size).
        seed: Random seed for reproducibility.
        min_count: Minimum observations per level for convergence (default 100).
        min_pairs: Minimum total pairs to sample before stopping (default 1,000,000).
            If the dataset is too small to generate this many pairs, estimation
            will proceed with all available pairs and log an info message.
        initial_batch_proportion: Starting batch size as row proportion (default 0.1).
        batch_growth_factor: Growth factor for row proportion each batch (default 2.0).

    Returns:
        List of ComparisonConvergenceInfo with diagnostics per comparison.
    """
    logger.info("----- Estimating u probabilities using random sampling -----")

    original_settings_obj = linker._settings_obj

    training_linker: Linker = deepcopy(linker)
    settings_obj = training_linker._settings_obj
    settings_obj._retain_matching_columns = False
    settings_obj._retain_intermediate_calculation_columns = False

    db_api = training_linker._db_api

    # Disable TF adjustments for u estimation
    for cc in settings_obj.comparisons:
        for cl in cc.comparison_levels:
            cl._tf_adjustment_column = None

    # Count records to determine sample size
    pipeline = CTEPipeline()
    pipeline = enqueue_df_concat(linker, pipeline)

    if settings_obj._link_type in ["dedupe_only", "link_and_dedupe"]:
        sql = """
        select count(*) as count
        from __splink__df_concat
        """
        pipeline.enqueue_sql(sql, "__splink__df_concat_count")
        count_dataframe = db_api.sql_pipeline_to_splink_dataframe(pipeline)
        result = count_dataframe.as_record_dict()
        count_dataframe.drop_table_from_database_and_remove_from_cache()

        total_nodes = result[0]["count"]
        sample_size = _rows_needed_for_n_pairs(max_pairs)
        proportion = sample_size / total_nodes

        # Calculate max possible pairs for this dataset
        max_possible_pairs = total_nodes * (total_nodes - 1) / 2

    if settings_obj._link_type == "link_only":
        sql = """
        select count(source_dataset) as count
        from __splink__df_concat
        group by source_dataset
        """
        pipeline.enqueue_sql(sql, "__splink__df_concat_count")
        counts_dataframe = db_api.sql_pipeline_to_splink_dataframe(pipeline)
        result = counts_dataframe.as_record_dict()
        counts_dataframe.drop_table_from_database_and_remove_from_cache()
        frame_counts = [res["count"] for res in result]

        proportion, sample_size = _proportion_sample_size_link_only(
            frame_counts, max_pairs
        )
        total_nodes = sum(frame_counts)

        # Calculate max possible pairs for link_only
        max_possible_pairs = (
            sum(frame_counts) ** 2 - sum([count**2 for count in frame_counts])
        ) / 2

    if proportion >= 1.0:
        proportion = 1.0
    if sample_size > total_nodes:
        sample_size = total_nodes

    # Check if dataset can achieve min_pairs and log info if not
    if max_possible_pairs < min_pairs:
        logger.info(
            f"Dataset can generate at most {max_possible_pairs:,.0f} pairs, "
            f"which is less than min_pairs={min_pairs:,}. "
            f"Proceeding with all available pairs."
        )

    # Order table for reproducibility when seed is provided
    table_to_sample_from = "__splink__df_concat"
    if seed is not None:
        uid_colname = settings_obj.column_info_settings.unique_id_input_column.name
        table_to_sample_from = (
            f"(select * from {table_to_sample_from} order by {uid_colname})"
        )

    # Create sampled table with deterministic random column for batching
    pipeline = CTEPipeline()
    pipeline = enqueue_df_concat(training_linker, pipeline)

    rand_sql = _generate_row_random_sql(
        settings_obj.column_info_settings.unique_id_input_columns,
        seed,
        db_api,
    )

    sql = f"""
    select *, {rand_sql} as __splink_u_rand
    from {table_to_sample_from}
    {training_linker._random_sample_sql(proportion, sample_size, seed)}
    """
    pipeline.enqueue_sql(sql, "__splink__df_concat_sample_with_rand")
    df_sample_with_rand = db_api.sql_pipeline_to_splink_dataframe(pipeline)

    # Set up table names
    settings_obj._blocking_rules_to_generate_predictions = []
    left_table = "__splink__df_concat_sample_with_rand"
    right_table = "__splink__df_concat_sample_with_rand"

    df_left: Optional[SplinkDataFrame] = None
    df_right: Optional[SplinkDataFrame] = None

    # Handle link_only with 2 tables
    if (
        len(linker._input_tables_dict) == 2
        and linker._settings_obj._link_type == "link_only"
    ):
        sqls = split_df_concat_with_tf_into_two_tables_sqls(
            "__splink__df_concat_sample_with_rand",
            linker._settings_obj.column_info_settings.source_dataset_column_name,
            sample_switch=False,
        )
        left_table = "__splink__df_concat_sample_with_rand_left"
        right_table = "__splink__df_concat_sample_with_rand_right"

        pipeline_left = CTEPipeline(input_dataframes=[df_sample_with_rand])
        pipeline_left.enqueue_sql(sqls[0]["sql"], sqls[0]["output_table_name"])
        df_left = db_api.sql_pipeline_to_splink_dataframe(pipeline_left)

        pipeline_right = CTEPipeline(input_dataframes=[df_sample_with_rand])
        pipeline_right.enqueue_sql(sqls[1]["sql"], sqls[1]["output_table_name"])
        df_right = db_api.sql_pipeline_to_splink_dataframe(pipeline_right)

    uid_columns = settings_obj.column_info_settings.unique_id_input_columns
    source_ds_col = settings_obj.column_info_settings.source_dataset_input_column
    uid_col = settings_obj.column_info_settings.unique_id_input_column

    # Calculate initial batch proportion if not explicitly provided
    if initial_batch_proportion is None:
        if max_pairs < 100_000:
            # For small max_pairs, skip batching and process all rows in one go
            initial_batch_proportion = 1.0
        else:
            # Start with a batch that targets min_pairs
            # Use the precise formula to calculate rows needed for target pairs
            #
            # We add a small 10% buffer to account for minor hash variance
            first_batch_target_pairs = min_pairs * 1.01
            rows_for_target = _rows_needed_for_n_pairs(first_batch_target_pairs)
            # Calculate proportion of our sampled dataset needed
            # Note: sample_size is the actual size of our sampled dataset
            initial_batch_proportion = rows_for_target / sample_size
            # Clamp to [0.01, 1.0] range
            initial_batch_proportion = max(0.01, min(1.0, initial_batch_proportion))

    # Estimate u for each comparison with convergence
    all_diagnostics: List[ComparisonConvergenceInfo] = []

    for i, comparison in enumerate(settings_obj.comparisons):
        original_comparison = original_settings_obj.comparisons[i]

        logger.warning(
            f"  Estimating u for comparison: {comparison.output_column_name}"
        )

        diagnostics = _estimate_u_with_row_batching(
            comparison=comparison,
            db_api=db_api,
            df_sample_with_rand=df_sample_with_rand,
            left_table_name=left_table,
            right_table_name=right_table,
            uid_input_columns=uid_columns,
            source_ds_column=source_ds_col,
            uid_column=uid_col,
            df_left=df_left,
            df_right=df_right,
            target_count_per_level=min_count,
            min_pairs=min_pairs,
            starting_row_proportion=initial_batch_proportion,
            row_proportion_growth=batch_growth_factor,
            link_type=linker._settings_obj._link_type,
        )

        all_diagnostics.append(diagnostics)

        # Log convergence info
        status = "converged" if diagnostics.converged else "NOT CONVERGED"
        logger.warning(
            f"    {status} after {diagnostics.batches_processed} batches, "
            f"{diagnostics.total_pairs_sampled:,} pairs, "
            f"{diagnostics.time_seconds:.2f}s"
        )

        # Build m_u_records from diagnostics for the existing interface
        m_u_records = [
            {
                "output_column_name": comparison.output_column_name,
                "comparison_vector_value": level.comparison_vector_value,
                "u_probability": level.u_probability,
                "m_probability": None,  # Not estimated here
            }
            for level in diagnostics.levels
        ]

        m_u_records_lookup = m_u_records_to_lookup_dict(m_u_records)

        for cl in original_comparison._comparison_levels_excluding_null:
            append_u_probability_to_comparison_level_trained_probabilities(
                cl,
                m_u_records_lookup,
                original_comparison.output_column_name,
                "estimate u by random sampling",
            )

    # Cleanup
    df_sample_with_rand.drop_table_from_database_and_remove_from_cache()
    if df_left is not None:
        df_left.drop_table_from_database_and_remove_from_cache()
    if df_right is not None:
        df_right.drop_table_from_database_and_remove_from_cache()

    logger.info("\nEstimated u probabilities using random sampling")

    return all_diagnostics
